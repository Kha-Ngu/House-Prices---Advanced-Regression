# House Prices â€“ Advanced Regression Techniques

Predict residential home sale prices in Ames, Iowa using creative feature engineering and advanced regression models (Random Forests, Gradient Boosting, XGBoost).

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)  
2. [Project Structure](#project-structure)  
3. [Getting Started](#getting-started)  
4. [Data](#data)  
5. [Methodology](#methodology)  
6. [Usage](#usage)  
7. [Results & Evaluation](#results--evaluation)  
8. [Submission Format](#submission-format)  
9. [Acknowledgments](#acknowledgments)  

---

## ğŸ” Overview

This project tackles the Kaggle â€œHouse Prices â€“ Advanced Regression Techniquesâ€ challenge:

> A playground competition with 79 explanatory variables describing almost every aspect of residential homes in Ames, Iowa. The goal is to predict each homeâ€™s final sale price.  

This project is designed to learn:

- Performing **creative feature engineering**  
- Training and tuning **advanced regression models** (Random Forest, XGBoost, Gradient Boosting)  
- Evaluating on **Root-Mean-Squared-Error (RMSE)** of log-prices  

---

## ğŸ—‚ Project Structure
- â”œâ”€â”€ data_description.txt # Data dictionary for all features
- â”œâ”€â”€ train.csv # Training set (with SalePrice)
- â”œâ”€â”€ test.csv # Test set (without SalePrice)
- â”œâ”€â”€ sample_submission.csv # Example submission format
- â”œâ”€â”€ output.csv # Final model predictions
- â””â”€â”€ Advanced_Regression.ipynb # Jupyter notebook walkthrough


---

## ğŸ“Š Data
- train.csv â€“ 1,460 samples with SalePrice.
- test.csv â€“ 1,459 samples without SalePrice.
- sample_submission.csv â€“ Id,SalePrice format.
- A complete feature dictionary is provided in data_description.txt.

## ğŸ›  Methodology
- Exploratory Data Analysis
- Identify missingness, distributions, correlations.
- Visualize with histograms, heatmaps.
- Feature Engineering
- Impute or flag missing values.
- Transform skewed numeric features (e.g., log, Boxâ€“Cox).
- Encode categoricals (Ordinal, One-Hot).
- Model Training
- Linear Regression (baseline).
- Random Forest Regressor â€“ tree-based ensemble.
- XGBoost Regressor â€“ gradient boosting with regularization.
- Hyperparameter Tuning
- Grid search / randomized search on key parameters.
- Cross-validation to guard against overfitting.
- Ensembling (optional)
- Blend predictions of multiple models for robustness.

## â–¶ï¸ Usage
### Open the Jupyter notebook:
jupyter notebook Advanced_Regression.ipynb

### Run cells in order to:
- Load data (train.csv, test.csv)
- Perform EDA & feature engineering
- Train and evaluate models
- Generate output.csv with final predictions
- Submit output.csv to the Kaggle competition for leaderboard scoring.

## ğŸ† Results & Evaluation
- Metric: RMSE of log(predicted) vs. log(actual) sale prices.
- Baseline (Linear Regression): ~0.160 RMSE
- Random Forest: ~0.112 RMSE
- XGBoost: ~0.105 RMSE
- Ensemble: ~0.101 RMSE

## ğŸ“‚ Submission Format
- csv
- Id,SalePrice
- 1461,169000.1
- 1462,187724.1233
- 1463,175221
â€¦
See output.csv for an example.

## ğŸ™ Acknowledgments
- Dataset: Ames Housing compiled by Dean De Cock (alternative to Boston Housing).
- Photo: â€œHouse in Ames, Iowaâ€ by Tom Thain on Unsplash.
- Tutorials & Inspiration: Kaggle Learn, community kernels on advanced regression techniques.
